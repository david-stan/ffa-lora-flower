{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_name_or_path = \"roberta-large\"\n",
    "dataset_name_or_path = \"stanfordnlp/snli\"\n",
    "task = \"snli\"\n",
    "peft_type = PeftType.LORA\n",
    "device = \"cuda\"\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(dataset_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    do_lower_case=False,\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters count: 108,312,579\n",
      "Trainable parameters count: 7,680,771\n"
     ]
    }
   ],
   "source": [
    "trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        total_params += p.numel()\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = True\n",
    "        trainable_params += p.numel()\n",
    "\n",
    "print(f\"Total parameters count: {total_params:,}\") # ~108M\n",
    "print(f\"Trainable parameters count: {trainable_params:,}\") # ~7M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
    "\n",
    "LABEL_LIST = [0, 1, 2]\n",
    "\n",
    "def _create_examples(dataset, set_type):\n",
    "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for index, item in enumerate(dataset):\n",
    "        if item['label'] not in LABEL_LIST:\n",
    "            continue\n",
    "        if not isinstance(item['premise'], str) or not isinstance(item['hypothesis'], str):\n",
    "            continue\n",
    "        guid = f\"{index}-{set_type}\"\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=item['premise'], text_b=item['hypothesis'], label=item['label']))\n",
    "    return examples\n",
    "\n",
    "def _dataset_to_features(dataset, set_type):\n",
    "    \"\"\" Pre-process text. This method will:\n",
    "    1) tokenize inputs\n",
    "    2) cut or pad each sequence to MAX_SEQ_LENGHT\n",
    "    3) convert tokens into ids\n",
    "\n",
    "    The output will contain:\n",
    "    `input_ids` - padded token ids sequence\n",
    "    `attention mask` - mask indicating padded tokens\n",
    "    `token_type_ids` - mask indicating the split between premise and hypothesis\n",
    "    `label` - label\n",
    "    \"\"\"\n",
    "    examples = _create_examples(dataset, set_type)\n",
    "\n",
    "    #backward compatibility with older transformers versions\n",
    "    legacy_kwards = {}\n",
    "    from packaging import version\n",
    "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
    "        legacy_kwards = {\n",
    "            \"pad_on_left\": False,\n",
    "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            \"pad_token_segment_id\": 0,\n",
    "        }\n",
    "\n",
    "    return glue_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        label_list=LABEL_LIST,\n",
    "        max_length=128,\n",
    "        output_mode=\"classification\",\n",
    "        **legacy_kwards,\n",
    "    )\n",
    "\n",
    "def _features_to_dataset(features):\n",
    "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
    "    \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_token_type_ids = torch.tensor(\n",
    "        [f.token_type_ids for f in features], dtype=torch.long\n",
    "    )\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_features = _dataset_to_features(datasets['train'], \"train\")\n",
    "test_features = _dataset_to_features(datasets['test'], \"test\")\n",
    "\n",
    "train_dataset = _features_to_dataset(train_features)\n",
    "test_dataset = _features_to_dataset(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_PHYSICAL_BATCH_SIZE = 8\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to train mode (HuggingFace models load in eval mode)\n",
    "model = model.train()\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LOGGING_INTERVAL = 2500 # once every how many steps we run evaluation cycle and report metrics\n",
    "EPSILON = 3.0\n",
    "DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "# define evaluation cycle\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            labels = inputs['labels'].detach().cpu().numpy()\n",
    "\n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(loss_arr), np.mean(accuracy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    target_delta=DELTA,\n",
    "    target_epsilon=EPSILON,\n",
    "    epochs=EPOCHS,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader,\n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'token_type_ids': batch[2],\n",
    "                    'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"É›: {eps:.2f}\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
