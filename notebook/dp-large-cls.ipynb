{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "model_name_or_path = \"roberta-large\"\n",
    "dataset_name_or_path = \"stanfordnlp/snli\"\n",
    "task = \"snli\"\n",
    "peft_type = PeftType.LORA\n",
    "device = \"cuda\"\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(dataset_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_skip_modules=[\"classifier\"]\n",
    ")\n",
    "\n",
    "model_name = \"FacebookAI/roberta-large\"\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    do_lower_case=False,\n",
    ")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"FacebookAI/roberta-large\",\n",
    "    config=config,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.data.processors.utils import InputExample\n",
    "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
    "\n",
    "LABEL_LIST = [0, 1, 2]\n",
    "\n",
    "def _create_examples(dataset, set_type):\n",
    "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for index, item in enumerate(dataset):\n",
    "        if item['label'] not in LABEL_LIST:\n",
    "            continue\n",
    "        if not isinstance(item['premise'], str) or not isinstance(item['hypothesis'], str):\n",
    "            continue\n",
    "        guid = f\"{index}-{set_type}\"\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=item['premise'], text_b=item['hypothesis'], label=item['label']))\n",
    "    return examples\n",
    "\n",
    "def _dataset_to_features(dataset, set_type):\n",
    "    \"\"\" Pre-process text. This method will:\n",
    "    1) tokenize inputs\n",
    "    2) cut or pad each sequence to MAX_SEQ_LENGHT\n",
    "    3) convert tokens into ids\n",
    "\n",
    "    The output will contain:\n",
    "    `input_ids` - padded token ids sequence\n",
    "    `attention mask` - mask indicating padded tokens\n",
    "    `token_type_ids` - mask indicating the split between premise and hypothesis\n",
    "    `label` - label\n",
    "    \"\"\"\n",
    "    examples = _create_examples(dataset, set_type)\n",
    "\n",
    "    #backward compatibility with older transformers versions\n",
    "    legacy_kwards = {}\n",
    "    from packaging import version\n",
    "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
    "        legacy_kwards = {\n",
    "            \"pad_on_left\": False,\n",
    "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            \"pad_token_segment_id\": 0,\n",
    "        }\n",
    "\n",
    "    return glue_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        label_list=LABEL_LIST,\n",
    "        max_length=128,\n",
    "        output_mode=\"classification\",\n",
    "        **legacy_kwards,\n",
    "    )\n",
    "\n",
    "def _features_to_dataset(features):\n",
    "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
    "    \"\"\"\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(\n",
    "        [f.attention_mask for f in features], dtype=torch.long\n",
    "    )\n",
    "    # all_token_type_ids = torch.tensor(\n",
    "    #     [f.token_type_ids for f in features], dtype=torch.long\n",
    "    # )\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids, all_attention_mask, all_labels\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_features = _dataset_to_features(datasets['train'], \"train\")\n",
    "test_features = _dataset_to_features(datasets['test'], \"test\")\n",
    "\n",
    "train_dataset = _features_to_dataset(train_features)\n",
    "test_dataset = _features_to_dataset(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputFeatures(input_ids=[0, 250, 621, 15, 10, 5253, 13855, 81, 10, 3187, 159, 16847, 4, 2, 2, 250, 621, 16, 1058, 39, 5253, 13, 10, 1465, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "MAX_PHYSICAL_BATCH_SIZE = 64\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters count: 53,151,747\n",
      "Total trainable parameters with LoRA: 1,839,107\n",
      "Total trainable parameters with LoRA after freezing matrix A: 393,216\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters count: {total_params:,}\")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # our particular task is sequence classification\n",
    "    inference_mode=False,  # Enable training mode\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=8,  # Alpha scaling factor\n",
    "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "    target_modules=[\"query\", \"value\"],\n",
    ")\n",
    "\n",
    "model_with_lora = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters with LoRA: {trainable_params:,}\")\n",
    "\n",
    "# classification_head = model_with_lora.base_model.model.classifier[:]\n",
    "\n",
    "# for param in classification_head.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# FFA-LoRA modification: freeze all adapter A matrices so that only B matrices are trainable\n",
    "for name, param in model_with_lora.named_parameters():\n",
    "    if \"lora_A\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# for name, param in model_with_lora.named_parameters():\n",
    "#     if \"classifier\" in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters with LoRA after freezing matrix A: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LOGGING_INTERVAL = 800 # once every how many steps we run evaluation cycle and report metrics\n",
    "EPSILON = 1.0\n",
    "DELTA = 1e-5 # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# def accuracy(preds, labels):\n",
    "#     return (preds == labels).mean()\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    # (preds == labels) returns a boolean tensor. Convert it to float and take the mean.\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "# define evaluation cycle\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                    #   'token_type_ids': batch[2],\n",
    "                      'labels':         batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            labels = inputs['labels']#.detach().cpu().numpy()\n",
    "\n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = sum(loss_arr) / len(loss_arr)\n",
    "    avg_accuracy = sum(accuracy_arr) / len(accuracy_arr)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "MAX_GRAD_NORM = 2.0\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, eps=1e-8)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "model = model_with_lora\n",
    "model = model.train()\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    target_delta=DELTA,\n",
    "    target_epsilon=EPSILON,\n",
    "    epochs=EPOCHS,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a785a190f2cc4756b02823121e097f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david-stan/workspace/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 800 | Train loss: 1.105 | Eval loss: 1.100 | Eval accuracy: 0.327 | É›: 0.34 \n",
      "Epoch: 1 | Step: 1600 | Train loss: 1.105 | Eval loss: 1.097 | Eval accuracy: 0.362 | É›: 0.42 \n",
      "Epoch: 1 | Step: 2400 | Train loss: 1.104 | Eval loss: 1.093 | Eval accuracy: 0.379 | É›: 0.49 \n",
      "Epoch: 1 | Step: 3200 | Train loss: 1.101 | Eval loss: 1.059 | Eval accuracy: 0.447 | É›: 0.53 \n",
      "Epoch: 1 | Step: 4000 | Train loss: 1.083 | Eval loss: 0.879 | Eval accuracy: 0.558 | É›: 0.57 \n",
      "Epoch: 1 | Step: 4800 | Train loss: 1.051 | Eval loss: 0.838 | Eval accuracy: 0.637 | É›: 0.60 \n",
      "Epoch: 1 | Step: 5600 | Train loss: 1.017 | Eval loss: 0.813 | Eval accuracy: 0.687 | É›: 0.63 \n",
      "Epoch: 1 | Step: 6400 | Train loss: 0.983 | Eval loss: 0.688 | Eval accuracy: 0.750 | É›: 0.66 \n",
      "Epoch: 1 | Step: 7200 | Train loss: 0.951 | Eval loss: 0.613 | Eval accuracy: 0.792 | É›: 0.68 \n",
      "Epoch: 1 | Step: 8000 | Train loss: 0.922 | Eval loss: 0.598 | Eval accuracy: 0.811 | É›: 0.70 \n",
      "Epoch: 1 | Step: 8800 | Train loss: 0.896 | Eval loss: 0.563 | Eval accuracy: 0.832 | É›: 0.72 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0e6996e7e64ad1a34d8173b6f098be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Step: 800 | Train loss: 0.630 | Eval loss: 0.551 | Eval accuracy: 0.846 | É›: 0.75 \n",
      "Epoch: 2 | Step: 1600 | Train loss: 0.628 | Eval loss: 0.554 | Eval accuracy: 0.849 | É›: 0.77 \n",
      "Epoch: 2 | Step: 2400 | Train loss: 0.625 | Eval loss: 0.559 | Eval accuracy: 0.855 | É›: 0.78 \n",
      "Epoch: 2 | Step: 3200 | Train loss: 0.622 | Eval loss: 0.558 | Eval accuracy: 0.856 | É›: 0.80 \n",
      "Epoch: 2 | Step: 4000 | Train loss: 0.620 | Eval loss: 0.566 | Eval accuracy: 0.856 | É›: 0.81 \n",
      "Epoch: 2 | Step: 4800 | Train loss: 0.619 | Eval loss: 0.540 | Eval accuracy: 0.862 | É›: 0.82 \n",
      "Epoch: 2 | Step: 5600 | Train loss: 0.617 | Eval loss: 0.539 | Eval accuracy: 0.865 | É›: 0.84 \n",
      "Epoch: 2 | Step: 6400 | Train loss: 0.615 | Eval loss: 0.541 | Eval accuracy: 0.865 | É›: 0.85 \n",
      "Epoch: 2 | Step: 7200 | Train loss: 0.614 | Eval loss: 0.537 | Eval accuracy: 0.867 | É›: 0.86 \n",
      "Epoch: 2 | Step: 8000 | Train loss: 0.612 | Eval loss: 0.537 | Eval accuracy: 0.867 | É›: 0.87 \n",
      "Epoch: 2 | Step: 8800 | Train loss: 0.611 | Eval loss: 0.536 | Eval accuracy: 0.868 | É›: 0.88 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a59130d418344498113121a51657c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8583 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Step: 800 | Train loss: 0.585 | Eval loss: 0.531 | Eval accuracy: 0.872 | É›: 0.90 \n",
      "Epoch: 3 | Step: 1600 | Train loss: 0.583 | Eval loss: 0.539 | Eval accuracy: 0.871 | É›: 0.91 \n",
      "Epoch: 3 | Step: 2400 | Train loss: 0.586 | Eval loss: 0.532 | Eval accuracy: 0.871 | É›: 0.92 \n",
      "Epoch: 3 | Step: 3200 | Train loss: 0.587 | Eval loss: 0.529 | Eval accuracy: 0.872 | É›: 0.93 \n",
      "Epoch: 3 | Step: 4000 | Train loss: 0.587 | Eval loss: 0.523 | Eval accuracy: 0.874 | É›: 0.94 \n",
      "Epoch: 3 | Step: 4800 | Train loss: 0.586 | Eval loss: 0.520 | Eval accuracy: 0.873 | É›: 0.95 \n",
      "Epoch: 3 | Step: 5600 | Train loss: 0.585 | Eval loss: 0.528 | Eval accuracy: 0.871 | É›: 0.96 \n",
      "Epoch: 3 | Step: 6400 | Train loss: 0.585 | Eval loss: 0.520 | Eval accuracy: 0.874 | É›: 0.96 \n",
      "Epoch: 3 | Step: 7200 | Train loss: 0.583 | Eval loss: 0.519 | Eval accuracy: 0.873 | É›: 0.97 \n",
      "Epoch: 3 | Step: 8000 | Train loss: 0.584 | Eval loss: 0.511 | Eval accuracy: 0.874 | É›: 0.98 \n",
      "Epoch: 3 | Step: 8800 | Train loss: 0.583 | Eval loss: 0.511 | Eval accuracy: 0.876 | É›: 0.99 \n",
      "Epoch: 3 | Step: 9600 | Train loss: 0.582 | Eval loss: 0.513 | Eval accuracy: 0.877 | É›: 1.00 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader,\n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    # 'token_type_ids': batch[2],\n",
    "                    'labels':         batch[2]}\n",
    "\n",
    "            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"É›: {eps:.2f} \"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
