# Description: Configuration file for the GSM8K task with the llama-7b model.
# Values are based on the paper "FFA-LoRA: Federated Fine-tuning with Adaptive Low-Rank Aggregation" per Section 5: Experiments.

task: gsm8k
model: llama-7b
num_clients: 3
batch_size: 200
num_rounds: 1000
local_epochs: 10
learning_rates:
  LoRA: [0.01, 0.02, 0.05, 0.1]
  FFA_LoRA: [0.1, 0.2, 0.5, 1.0]
rank: 8
scaling_factor: 8
privacy:
  enabled: true
  epsilon: [6, 3, 1]
  delta: 1e-5
  clipping_threshold: [2, 5, 10]
