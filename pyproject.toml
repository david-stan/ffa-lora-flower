[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "flowertune_llm"
version = "1.0.0"
description = "FlowerTune LLM: Federated LLM Fine-tuning with Flower"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]==1.15.0",
    "flwr-datasets>=0.5.0",
    "torch==2.3.1",
    "trl==0.8.1",
    "bitsandbytes==0.45.0",
    "scipy==1.13.0",
    "peft==0.6.2",
    "fschat[model_worker,webui]==0.2.35",
    "transformers==4.47.0",
    "sentencepiece==0.2.0",
    "omegaconf==2.3.0",
    "hf_transfer==0.1.8",
    "opacus==v1.4.1",
    "dp_transformers==1.0.1",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "flwrlabs"

[tool.flwr.app.components]
serverapp = "flowertune_llm.server_app:app"
clientapp = "flowertune_llm.client_app:app"

[tool.flwr.app.config]
dataset.name = "openai/gsm8k"
model.name = "huggyllama/llama-7b"
model.quantization = 4
model.gradient-checkpointing = true
model.lora.peft-lora-r = 8
model.lora.peft-lora-alpha = 8
train.save-every-round = 5
train.learning-rate-max = 5e-4
train.learning-rate-min = 1e-4
train.seq-length = 512
train.training-arguments.output-dir = ""
train.training-arguments.learning-rate = ""
train.training-arguments.per-device-train-batch-size = 32
train.training-arguments.gradient-accumulation-steps = 2
train.training-arguments.weight-decay = 0.01
train.training-arguments.logging-steps = 1
train.training-arguments.num-train-epochs = 3
train.training-arguments.max-steps = 6
train.training-arguments.save-steps = 1000
train.training-arguments.save-total-limit = 10
train.training-arguments.gradient-checkpointing = true
train.training-arguments.lr-scheduler-type = "constant"
train.training-arguments.fp16 = false
train.training-arguments.bf16 = true
train.training-arguments.tf32 = true
train.training-arguments.remove-unused-columns = false
strategy.fraction-fit = 1.0
strategy.fraction-evaluate = 0.0
privacy.target_epsilon = 6
privacy.target_delta = 1e-5
privacy.per_sample_max_grad_norm = 2.0
num-server-rounds = 60

[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.federations.local-simulation]
options.num-supernodes = 3
options.backend.client-resources.num-cpus = 8
options.backend.client-resources.num-gpus = 1.0